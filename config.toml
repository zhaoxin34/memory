# Memory Configuration File
# This is an example configuration file for the Memory knowledge base system.
# Copy this file to config.toml and customize for your needs.

# Application settings
app_name = "memory"
log_level = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
json_logs = false
# Note: data_dir will use default Path.home() / ""
# You can override with environment variable MEMORY_DATA_DIR
default_repository = "default" # Default repository for documents (can be overridden with MEMORY_DEFAULT_REPOSITORY)

# Embedding provider configuration
# Choose between local, OpenAI, or custom API embeddings
[embedding]
# Options:
# - "local": Use local sentence-transformers models
# - "openai": Use OpenAI-compatible API (supports custom base_url via extra_params)
# - "ollama": Use Ollama (OpenAI-compatible, uses extra_params.base_url)
# provider = "openai"
# For OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-v4, text-embedding-ada-002
# For local: all-MiniLM-L6-v2, all-mpnet-base-v2, BAAI/bge-small-zh-v1.5, BAAI/bge-large-zh-v1.5
# For Ollama: nomic-embed-text, mxbai-embed-large, bge-m3
# model_name = "text-embedding-v4"

provider = "ollama"
model_name = "bge-m3"

# API key is not required when using custom base_url (e.g., Ollama)
# api_key = "not-needed"
batch_size = 10

# For Ollama, set base_url to localhost:
[embedding.extra_params]
base_url = "http://localhost:11434/v1"

# LLM provider configuration (for ask command)
# Note: LLM providers are not yet implemented
[llm]
provider = "openai"      # Options: openai, anthropic (not yet implemented)
model_name = "qwen3-max"
# API key environment variable name (code will call os.getenv(api_key))
# Example: api_key = "BAILIAN_API_KEY" means the code will call os.getenv("BAILIAN_API_KEY")
api_key = "BAILIAN_API_KEY"
max_tokens = 2000
temperature = 0.7

# Vector store configuration
# Chroma is recommended for local persistent storage
[vector_store]
store_type = "chroma"      # Options: chroma, memory
collection_name = "memory"

# Chroma-specific settings (install with `uv sync --extra chroma`)
# Note: Path will be expanded (e.g., ~/.memory/chroma -> /Users/username/.memory/chroma)
[vector_store.extra_params]
persist_directory = "~/.memory/chroma"

# For in-memory storage (no persistence, useful for testing):
# [vector_store]
# store_type = "memory"
# collection_name = "memory"

# Metadata store configuration
[metadata_store]
store_type = "sqlite" # Options: sqlite
# Note: Path will be expanded (e.g., ~/.memory/metadata.db -> /Users/username/.memory/metadata.db)
connection_string = "~/.memory/metadata.db"
collection_name = "memory"

# Document chunking configuration
[chunking]
chunk_size = 1024
chunk_overlap = 50
min_chunk_size = 100

# ============================================================================
# Deployment profiles
# Use with: memory --profile <profile_name>
# ============================================================================

# Local development profile: Uses local embeddings and Chroma
[profiles.local]
log_level = "DEBUG"
[profiles.local.embedding]
provider = "local"
model_name = "all-MiniLM-L6-v2"
[profiles.local.vector_store]
store_type = "chroma"
[profiles.local.vector_store.extra_params]
# Note: Path will be expanded (e.g., ~/.memory/chroma -> /Users/username/.memory/chroma)
persist_directory = "~/.memory/chroma"

# OpenAI profile: Uses OpenAI embeddings with Chroma storage
[profiles.openai]
log_level = "INFO"
[profiles.openai.embedding]
provider = "openai"
model_name = "text-embedding-3-small"
# Note: Set OPENAI_API_KEY environment variable for this profile
# Or copy and modify this profile to use BAiLIAN API
api_key = "OPENAI_API_KEY"
[profiles.openai.vector_store]
store_type = "chroma"
[profiles.openai.vector_store.extra_params]
# Note: Path will be expanded (e.g., ~/.memory/chroma -> /Users/username/.memory/chroma)
persist_directory = "~/.memory/chroma"

# Testing profile: Uses in-memory storage (no persistence)
[profiles.test]
log_level = "DEBUG"
[profiles.test.embedding]
provider = "local"
model_name = "all-MiniLM-L6-v2"
[profiles.test.vector_store]
store_type = "memory"
[profiles.test.metadata_store]
store_type = "memory"

# Local BGE small model profile: Uses local embedding with Chroma
[profiles.bge-small]
log_level = "INFO"
[profiles.bge-small.embedding]
provider = "local"
model_name = "BAAI/bge-small-zh-v1.5"
[profiles.bge-small.vector_store]
store_type = "chroma"
[profiles.bge-small.vector_store.extra_params]
persist_directory = "~/.memory/chroma-bge-small"
[profiles.bge-small.metadata_store]
store_type = "sqlite"
connection_string = "~/.memory/metadata-bge-small.db"

# ============================================================================
# Installation Instructions
# ============================================================================
#
# Install dependencies based on your chosen providers:
#
# For local embeddings:
#   uv sync --extra local
#
# For OpenAI embeddings:
#   uv sync --extra openai
#
# For Chroma vector store:
#   uv sync --extra chroma
#
# Install multiple extras:
#   uv sync --extra local --extra chroma
#   uv sync --extra openai --extra chroma
#
# For development (includes all extras):
#   uv sync --extra dev --extra local --extra openai --extra chroma
#
